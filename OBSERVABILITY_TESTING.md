# ğŸ§ª Observability Testing Guide

GuÃ­a para generar datos de prueba en Logfire y Sentry.

---

## ğŸ¯ Objetivo

El script `test_observability.py` genera trÃ¡fico controlado para:

1. **Logfire:** Traces, mÃ©tricas y logs estructurados
2. **Sentry:** Errores controlados y contexto de debugging

---

## ğŸš€ Uso BÃ¡sico

### InstalaciÃ³n de Dependencias

```bash
# AsegÃºrate de tener httpx instalado
uv pip install httpx
```

### Ejecutar Tests

```bash
# Test bÃ¡sico en staging (3 scans)
python test_observability.py --env staging

# MÃ¡s scans para mÃ¡s datos
python test_observability.py --env staging --count 10

# Test en development
python test_observability.py --env development

# Test local (si tienes servidor corriendo)
python test_observability.py --env local
```

---

## ğŸ“Š Tests Incluidos

### 1. Health Check
- âœ… Verifica que `/health` funciona
- âœ… Genera trace simple en Logfire
- âœ… Confirma que observability estÃ¡ activa

### 2. Successful Scans
- âœ… Ejecuta scans reales con diferentes marcas
- âœ… Genera traces completos en Logfire (incluye Gemini, Redis, DB)
- âœ… Muestra mÃ©tricas de performance (tiempo de respuesta)

### 3. Error Scenarios
- âœ… Genera errores controlados (422, 404)
- âœ… EnvÃ­a eventos a Sentry con contexto
- âœ… Ãštil para probar alertas y notificaciones

### 4. Docs Endpoint
- âœ… Verifica que `/docs` estÃ¡ accesible
- âœ… Genera trace adicional

### 5. Concurrent Requests
- âœ… Ejecuta mÃºltiples requests simultÃ¡neos
- âœ… Prueba capacidad de carga
- âœ… Genera mÃºltiples traces paralelos

---

## ğŸ›ï¸ Opciones Avanzadas

```bash
# Especificar nÃºmero de scans
python test_observability.py --env staging --count 5

# Especificar requests concurrentes
python test_observability.py --env staging --concurrent 10

# Saltar tests de errores (solo traces exitosos)
python test_observability.py --env staging --skip-errors

# Combinar opciones
python test_observability.py --env staging --count 10 --concurrent 5
```

---

## ğŸ“ˆ QuÃ© Esperar en Dashboards

### Logfire Dashboard

**DespuÃ©s de ejecutar tests, deberÃ­as ver:**

1. **Traces:**
   - `GET /health` (1 trace)
   - `GET /?brand=X` (N traces, uno por scan)
   - `GET /docs` (1 trace)

2. **MÃ©tricas:**
   - Request count
   - Response time (p50, p95, p99)
   - Error rate

3. **Spans:**
   - FastAPI request handling
   - Gemini API calls
   - Redis cache operations
   - Database queries

**URL:** https://logfire.pydantic.dev

---

### Sentry Dashboard

**DespuÃ©s de ejecutar tests, deberÃ­as ver:**

1. **Issues:**
   - `422 Validation Error` (missing/invalid brand parameter)
   - `404 Not Found` (non-existent endpoint)

2. **Performance:**
   - Transaction traces de requests exitosos
   - Response times

3. **Context:**
   - Environment (staging/development)
   - Request parameters
   - Stack traces

**URL:** https://sentry.io

---

## ğŸ” VerificaciÃ³n

### 1. Ejecutar Tests

```bash
python test_observability.py --env staging --count 5
```

### 2. Esperar 1-2 minutos

Los datos pueden tardar unos segundos en aparecer en los dashboards.

### 3. Verificar Logfire

```
1. Ir a: https://logfire.pydantic.dev
2. Seleccionar proyecto: compas-scan
3. Ver "Traces" tab
4. Filtrar por Ãºltimos 5 minutos
5. DeberÃ­as ver mÃºltiples traces de GET requests
```

### 4. Verificar Sentry

```
1. Ir a: https://sentry.io
2. Seleccionar proyecto: compas-scan
3. Ver "Issues" tab
4. DeberÃ­as ver errores 422 y 404
5. Ver "Performance" tab para traces
```

---

## ğŸ¯ Casos de Uso

### Generar Datos para Demo

```bash
# Generar 20 scans para demo completa
python test_observability.py --env staging --count 20 --concurrent 5
```

### Testing de Performance

```bash
# Stress test con 50 requests concurrentes
python test_observability.py --env staging --count 50 --concurrent 50
```

### Testing de Errores

```bash
# Solo errores (sin scans exitosos)
python test_observability.py --env staging --count 0 --skip-errors false
```

---

## ğŸ› Troubleshooting

### "Connection refused" o Timeout

**Problema:** El ambiente no estÃ¡ disponible o tiene protection activada.

**SoluciÃ³n:**
```bash
# Verificar que el ambiente estÃ© desplegado
curl https://compas-scan-staging.vercel.app/health

# Si retorna "Authentication Required", deshabilitar protection en Vercel
```

### No aparecen datos en Logfire

**Problema:** LOGFIRE_TOKEN no configurado o invÃ¡lido.

**SoluciÃ³n:**
1. Verificar que `LOGFIRE_TOKEN` estÃ© en Vercel Dashboard
2. Verificar que el token sea vÃ¡lido
3. Revisar logs del deployment en Vercel

### No aparecen errores en Sentry

**Problema:** SENTRY_DSN no configurado o errores no se estÃ¡n capturando.

**SoluciÃ³n:**
1. Verificar que `SENTRY_DSN` estÃ© en Vercel Dashboard
2. Los errores 422 y 404 son esperados (no son crÃ­ticos)
3. Revisar que Sentry estÃ© inicializado correctamente

---

## ğŸ“ Ejemplo de Output

```
ğŸ§ª Observability Testing - STAGING
â„¹ï¸  Target: https://compas-scan-staging.vercel.app
â„¹ï¸  Scans: 3
â„¹ï¸  Concurrent: 3

â„¹ï¸  Testing Health Check endpoint...
âœ… Health Check OK: healthy
â„¹ï¸     Observability: {
  "logfire": true,
  "sentry": true
}

============================================================
Testing 3 Successful Scans (Logfire Traces)
============================================================

â„¹ï¸  [1/3] Scanning: Nike
   âœ… Scan OK (2.45s)
   Status: success
   Found: 3 HDA, 5 LDA competitors

â„¹ï¸  [2/3] Scanning: Adidas
   âœ… Scan OK (2.12s)
   Status: success
   Found: 2 HDA, 4 LDA competitors

...

============================================================
ğŸ“Š Test Summary
============================================================

Total Tests: 12
Successful: 11
Failed: 1

ğŸ“ˆ Expected Observability Data:
  â€¢ Logfire: 8 traces (health + scans + docs)
  â€¢ Sentry: 3 error events (controlled errors)
  â€¢ Metrics: Request counts, response times, error rates

ğŸ” Check your dashboards:
  â€¢ Logfire: https://logfire.pydantic.dev
  â€¢ Sentry: https://sentry.io

âœ… Testing Complete!
Check your observability dashboards in 1-2 minutes for new data.
```

---

## ğŸ‰ Â¡Listo!

Ahora tienes un script completo para generar datos de prueba en tus dashboards de observabilidad.

**PrÃ³ximos pasos:**
1. Ejecutar tests regularmente para mantener dashboards activos
2. Usar en CI/CD para testing automatizado
3. Personalizar marcas de prueba segÃºn tus necesidades

